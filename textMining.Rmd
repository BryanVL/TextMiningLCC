---
title: "Proyecto de Text Mining"
author: "Bryan Velicka Leka y Franco Manuel García Dos Santos"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ---------------------- Introducción ---------------------- #

En este documento haremos text mining sobre los pdfs dados y haremos un pequeño
trabajo de investigación.


## 0. Carga de paquetes necesarios
```{r message=FALSE, warning=FALSE}
library(tm)
library(pdftools)
library(stringr)
library(stringi)
library(wordcloud)
library(ggplot2)

```


## 1. Importación de textos
```{r}
#Directorio en el que estan los pdfs
directorio.textos <- file.path("./","pdfstextmining2021")
directorio.textos

#Listar ficheros en el directorio dado
dir(directorio.textos)

#Crea un objeto fuente (para acceder a los pdfs)
list.files <- DirSource(directorio.textos)

#aplica la funcion pdf_text a todos los pdfs de la fuente.
#Esta función convierte los pdfs en texto
texts <- lapply(list.files, pdf_text)

#Longitud de la variable texts. Esto es el numero 
#de textos distintos que tenemos
length(texts)

#Devuelve la longitud de todos los elementos de texts
lapply(texts, length)

#Crea un corpus a partir de texts
corpus <- VCorpus(VectorSource(texts))
inspect(corpus)$meta

#Esta función quita signos de puntuación, espacios en blanco, convierte a 
#minuscula y quita palabras no significativas en ingles.
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation, ucp = TRUE)
  corpus <- tm_map(corpus,stripWhitespace)
  corpus <- tm_map(corpus,content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en")) )
  return(corpus)
}

corpus <- clean_corpus(corpus)

```


## 2. Creación de Contenedor para eliminar caracteres especiales
```{r}
#Esta funcion recibe un patron y convierte en espacio en blanco las coincidencias
removeSpCh <- content_transformer(function (x , pattern ) gsub(pattern, " ", x) )

#Esta funcion recibe un patron y convierte las coincidencias en lo que 
#reciba en su segundo argumento
sustitution <- content_transformer(function (x , pattern, sustitucion ) gsub(pattern, sustitucion, x) )

#Eliminamos caracteres especiales
corpus <- tm_map(corpus, removeSpCh, "\a") #Eliminar caracter \a
corpus <- tm_map(corpus, removeSpCh, "\n")
corpus <- tm_map(corpus, removeSpCh, "\f")

#Eliminamos palabras que no nos interesan
#Eliminar todas las palabras que contengan http
corpus <- tm_map(corpus, removeSpCh, "(http)") 
corpus <- tm_map(corpus, removeSpCh, "(aaas)")
corpus <- tm_map(corpus, removeSpCh, "(doi)")
corpus <- tm_map(corpus, removeSpCh, "(also)")
corpus <- tm_map(corpus, removeSpCh, "(can)")

#Sustituir empt por empty
corpus <- tm_map(corpus, sustitution, "(empt)", "empty")

```


## 3. Creación de TDM/DTM y inspección de los mismos
```{r}
#Esta función convierte un corpus en un tdm con un determinado formato
to_TDM <- function(my_corpus){
    my_corpus <- TermDocumentMatrix(my_corpus,
                                    control = list( removePunctuation = TRUE,
                                              stopwords = stopwords(kind = "en"),
                                              tolower = TRUE,
                                              stemming = FALSE,
                                              removeNumbers = TRUE) )
}

#Esta función convierte un corpus en un dtm con un determinado formato
to_DTM <- function(my_corpus){
    my_corpus <- DocumentTermMatrix(my_corpus,
                                    control = list( removePunctuation = TRUE,
                                              stopwords = stopwords(kind = "en"),
                                              tolower = TRUE,
                                              stemming = FALSE,
                                              removeNumbers = TRUE) )
}

#Convertimos el corpus en un TDM
my_tdm <- to_TDM(corpus)

#Buscamos los terminos mas frecuentes. Como minimo deben tener frecuencia 20
frequent_terms <-  findFreqTerms(my_tdm, lowfreq = 20, highfreq = Inf)
frequent_terms

#Convertimos los terminos frecuentes del TDM en una matriz y vemos cuantas
#veces aparece cada palabra en cada texto
matrix_tdm <- as.matrix(my_tdm[frequent_terms,])
matrix_tdm

#Se suman las filas para conseguir la frecuencia de cada termino
freq <- rowSums(matrix_tdm)
freq

#Se hace una nube de palabras al que se le pasa los nombres de los terminos, 
#las frecuencias, la frecuencia minima para que se muestre la palabra
#y el rango de tamaño de las palabras a enseñar
set.seed(142)   
wordcloud(names(freq), freq, min.freq=5, scale=c(3, .1))


#Se repite el procedimiento anterior pero esta vez usando un DTM en vez de un TDM
dtm <- to_DTM(corpus)
freq <- colSums(as.matrix(dtm))

#Se especifican ahora colores para las palabras
set.seed(142)   
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, min.freq=10, max.words = 50, scale=c(2.5, .1), colors = dark2)


#Mostrar palabras del corpus ordenadas por frecuencia (menor a mayor)
dtm <- to_DTM(corpus)
freq <- colSums(as.matrix(dtm))
ord <- order(freq)
head(freq[ord])

#Mostrar palabras del corpus ordenadas por frecuencia (mayor a menor)
tdm <- to_TDM(corpus)
freq <- rowSums(as.matrix(tdm))
ord <- order(freq, decreasing=TRUE)
head(freq[ord])


#Podemos crear también un grafico de barras de frecuencias
dtm <- to_DTM(corpus)
freq <- colSums(as.matrix(dtm))
wf <- data.frame(word=names(freq), freq=freq)

p <- ggplot(subset(wf, freq>30), aes(word, freq)) +
  geom_bar(stat="identity", fill="lightblue", colour="black") + 
  theme(axis.text.x=element_text(angle=45, hjust=1),
    axis.title = element_text(size = 12, face = "bold") ) +
  labs(x = "Palabras", y="Frecuencia") +
  geom_text( aes(label=freq), vjust=-0.3, colour="black", size = 3.5)

p

```


## 4. Buscar correlación entre palabras
```{r}
#Vamos a ver la correlación que tienen las palabras más comunes que parecen 
#mas interesantes con otras palabras. La correlación minima debe ser de 0.9
asociaciones <- findAssocs(dtm, c("covid" , "conspiracy", "coronavirus", "science", "public", "vaccine"), corlimit=0.9) 

asociaciones$covid
asociaciones$conspiracy
asociaciones$science
asociaciones$public
asociaciones$coronavirus[1:10]
asociaciones$vaccine[1:10]
```







