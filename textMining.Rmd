---
title: "Proyecto de Text Mining"
author: "Bryan Velicka Leka y Franco Manuel García Dos Santos"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ---------------------- Introducción ---------------------- #

En este documento haremos text mining sobre los pdfs dados y haremos un pequeño
trabajo de investigación.


## 0. Carga de paquetes necesarios
```{r message=FALSE, warning=FALSE}
library(tm)
library(pdftools)
library(stringr)
library(stringi)
library(wordcloud)
library(ggplot2)
library(tidyverse)
```


## 1. Importación de textos
```{r}
#Directorio en el que estan los pdfs
directorio.textos <- file.path("./","pdfstextmining2021")
directorio.textos

#Listar ficheros en el directorio dado
dir(directorio.textos)

#Crea un objeto fuente (para acceder a los pdfs)
list.files <- DirSource(directorio.textos)

#aplica la funcion pdf_text a todos los pdfs de la fuente.
#Esta función convierte los pdfs en texto
texts <- lapply(list.files, pdf_text)

#Longitud de la variable texts. Esto es el numero 
#de textos distintos que tenemos
length(texts)

#Devuelve la longitud de todos los elementos de texts
lapply(texts, length)

#Crea un corpus a partir de texts
corpus <- VCorpus(VectorSource(texts))
inspect(corpus)$meta

#Esta función quita signos de puntuación, espacios en blanco, convierte a 
#minuscula y quita palabras no significativas en ingles.
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation, ucp = TRUE)
  corpus <- tm_map(corpus,stripWhitespace)
  corpus <- tm_map(corpus,content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en")) )
  return(corpus)
}

corpus <- clean_corpus(corpus)

```


## 2. Creación de Contenedor para eliminar caracteres especiales
```{r}
#Esta funcion recibe un patron y convierte en espacio en blanco las coincidencias
removeSpCh <- content_transformer(function (x , pattern ) gsub(pattern, " ", x) )

#Esta funcion recibe un patron y convierte las coincidencias en lo que 
#reciba en su segundo argumento
sustitution <- content_transformer(function (x , pattern, sustitucion ) gsub(pattern, sustitucion, x) )

#Eliminamos caracteres especiales
corpus <- tm_map(corpus, removeSpCh, "\a") #Eliminar caracter \a
corpus <- tm_map(corpus, removeSpCh, "\n")
corpus <- tm_map(corpus, removeSpCh, "\f")

#Eliminamos palabras que no nos interesan
#Eliminar todas las palabras que contengan http
corpus <- tm_map(corpus, removeSpCh, "(http)") 
corpus <- tm_map(corpus, removeSpCh, "(aaas)")
corpus <- tm_map(corpus, removeSpCh, "(doi)")
corpus <- tm_map(corpus, removeSpCh, "(also)")
corpus <- tm_map(corpus, removeSpCh, "(can)")

#Sustituir empt por empty
corpus <- tm_map(corpus, sustitution, "(empt)", "empty")

```


## 3. Creación de TDM/DTM y inspección de los mismos
```{r}
#Esta función convierte un corpus en un tdm con un determinado formato
to_TDM <- function(my_corpus){
    my_corpus <- TermDocumentMatrix(my_corpus,
                                    control = list( removePunctuation = TRUE,
                                              stopwords = stopwords(kind = "en"),
                                              tolower = TRUE,
                                              stemming = FALSE,
                                              removeNumbers = TRUE) )
}

#Esta función convierte un corpus en un dtm con un determinado formato
to_DTM <- function(my_corpus){
    my_corpus <- DocumentTermMatrix(my_corpus,
                                    control = list( removePunctuation = TRUE,
                                              stopwords = stopwords(kind = "en"),
                                              tolower = TRUE,
                                              stemming = FALSE,
                                              removeNumbers = TRUE) )
}

#Convertimos el corpus en un TDM
my_tdm <- to_TDM(corpus)

#Buscamos los terminos mas frecuentes. Como minimo deben tener frecuencia 20
frequent_terms <-  findFreqTerms(my_tdm, lowfreq = 20, highfreq = Inf)
frequent_terms

#Convertimos los terminos frecuentes del TDM en una matriz y vemos cuantas
#veces aparece cada palabra en cada texto
matrix_tdm <- as.matrix(my_tdm[frequent_terms,])
matrix_tdm

#Se suman las filas para conseguir la frecuencia de cada termino
freq <- rowSums(matrix_tdm)
freq

#Se hace una nube de palabras al que se le pasa los nombres de los terminos, 
#las frecuencias, la frecuencia minima para que se muestre la palabra
#y el rango de tamaño de las palabras a enseñar
set.seed(142)   
wordcloud(names(freq), freq, min.freq=5, scale=c(3, .1))


#Se repite el procedimiento anterior pero esta vez usando un DTM en vez de un TDM
dtm <- to_DTM(corpus)
freq <- colSums(as.matrix(dtm))

#Se especifican ahora colores para las palabras
set.seed(142)   
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, min.freq=10, max.words = 50, scale=c(2.5, .1), colors = dark2)


#Mostrar palabras del corpus ordenadas por frecuencia (menor a mayor)
dtm <- to_DTM(corpus)
freq <- colSums(as.matrix(dtm))
ord <- order(freq)
head(freq[ord])

#Mostrar palabras del corpus ordenadas por frecuencia (mayor a menor)
tdm <- to_TDM(corpus)
freq <- rowSums(as.matrix(tdm))
ord <- order(freq, decreasing=TRUE)
head(freq[ord])


#Podemos crear también un grafico de barras de frecuencias
dtm <- to_DTM(corpus)
freq <- colSums(as.matrix(dtm))
wf <- data.frame(word=names(freq), freq=freq)

p <- ggplot(subset(wf, freq>30), aes(word, freq)) +
  geom_bar(stat="identity", fill="lightblue", colour="black") + 
  theme(axis.text.x=element_text(angle=45, hjust=1),
    axis.title = element_text(size = 12, face = "bold") ) +
  labs(x = "Palabras", y="Frecuencia") +
  geom_text( aes(label=freq), vjust=-0.3, colour="black", size = 3.5)

p

```


## 4. Buscar correlación entre palabras
```{r}
#Vamos a ver la correlación que tienen las palabras más comunes que parecen 
#mas interesantes con otras palabras. La correlación minima debe ser de 0.9
asociaciones <- findAssocs(dtm, c("covid" , "conspiracy", "coronavirus", "science", "public", "vaccine"), corlimit=0.9) 

asociaciones$covid
asociaciones$conspiracy
asociaciones$science
asociaciones$public
asociaciones$coronavirus[1:10]
asociaciones$vaccine[1:10]
```



## 5. Topic modeling 
```{r}
#El topic modeling es un metodo de clasificación no supervisada como lo puede
#ser el clustering que tiene el objetivo de agrupar de forma natural los
#documentos que se le pasen. Un metodo popular es Latent Dirichlet allocation (LDA).
#Este algoritmo se basa en que todos los documentos estan formados por distintos
#topicos y cada topico por distintas palabras. 

#Para usar el algoritmo LDA se puede usar la función LDA() del paquete topicmodels.

install.packages("topicmodels")
library(topicmodels)

#Esta funcion devuelve un conjunto de topicos. Usaremos el dtm antes generado
#un k=10 (siendo este el numero de topicos)
topicos <- LDA(dtm, k=6)


#Ahora podemos usar la funcion tidy de tidytext para inspecionar el objeto generado
#y ggplot para visualizarlo

install.packages("tidytext")
install.packages("reshape2")
library(tidytext)

ap_topics <- tidy(topicos, matrix="beta")
ap_topics

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#En estas graficas se muestran las palabras más comunes para cada uno de los 
#topicos calculados
ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

#Lo que podemos deducir de por ejemplo el grupo 1 es que trata sobre el tema
#de las vacunas para el coronavirus. El segundo sobre ciencia o cientificos, 
# el tercero sobre las muertes provocadas por el covid y el 5 y 6 
#sobre conspiraciones. 

#(Nota: las graficas se muestran en orden aleatorio y
#la descripción anterior puede no ajustarse a lo que se ve en las graficas)



#Otro paquete con el que es posible hacer topic modeling es stm
install.packages("stm")
library(stm)

#La funcion que tiene para sacar topicos se llama stm (structural equation modeling)
#y funciona de manera similar a la funcion LDA() usada anteriormente. Se le
#pasa como parametro un dfm (Document-feature matrix), 
#el numero de topicos a generar y el algoritmo a usar

#Primero convertimos nuestro dtm en un dfm
dfm <- tidy(dtm) %>% 
  cast_dfm(document = document, term = term, value = count)
dfm

topicos_stm <- stm(dfm, K=6, verbose = FALSE, init.type = "LDA")
topicos_stm


#Podemos usar la función summary para ver los topicos generados
summary(topicos_stm)

#Como hicimos anteriormente, usando la funcion tidy podemos ver las palabras
#que tienen más probabilidad de aparecer
graf <- tidy(topicos_stm) %>% 
  group_by(topic) %>%
  top_n(10) %>% 
  ungroup() %>% 
  mutate(topic = str_c("topic", topic))


ggplot(graf,aes(x = beta, y = term,  by = topic))+
geom_bar(aes(fill = "kat" ), position = "dodge", width=.5, stat="identity")
labs(title = "Words with highest probability in each topic")

  

  

#Por ultimo, otra forma de hacer topic modeling es con el paquete textmineR
install.packages("textmineR")
library(textmineR)

#Al igual que las dos alternativas anteriores, esta ofrece una función para 
#generar los topicos con LDA. En este caso la función se llama FitLdaModel y 
#recibe como parametros un DTM y el numero de topicos a generar (puede recibir
#muchos parametros extras que se pueden ver en la ayuda, siendo uno interesante
#el de cpus que permite indicar cuanto queremos paralelizar el proceso)


topicos_textminer <- FitLdaModel(dtm = dfm, 
                     k = 6,
                     iterations = 200, 
                     burnin = 180,
                     alpha = 0.1,
                     beta = 0.05,
                     optimize_alpha = TRUE,
                     calc_likelihood = TRUE,
                     calc_coherence = TRUE,
                     calc_r2 = TRUE,
                     cpus = 2) 
  
#Este metodo nos ofrece estadisticos para evaluar los modelos como el r ajustado
topicos_textminer$r2
topicos_textminer$theta
topicos_textminer$coherence
```




















